{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw5_Sima Jeddi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKIz_ttKiG_1"
      },
      "source": [
        "# IFT6269 - Homework 5 - Sampling Methods\n",
        "**Due:**  Tuesday, December 15, 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whO8SD53Y9Vl"
      },
      "source": [
        "#### Name: Sima Jeddi\n",
        "#### Student ID: \n",
        "#### Collaborators: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3gk4JHNY9yW"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(precision=5, suppress=True)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "#                       Code for plotting the results \n",
        "#                      ! DO NOT MODIFY THIS FUNCTION !\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def show_matrix(A):\n",
        "    matfig = plt.figure(figsize=(5,5))\n",
        "    plt.matshow(A, cmap=plt.cm.gray, fignum=matfig.number)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "def plot_list(data, title=\"\"):\n",
        "    plt.figure(figsize=(5,2))\n",
        "    plt.plot(data)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huBL0znH4Tii"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "0. Get your own copy of this file via \"File > Save a copy in Drive...\",\n",
        "1. Fill your personal information and collaborators at the top of this assignment, and rename the notebook accordingly, e.g., `hw5_thomasBayes.ipynb`\n",
        "2. Read the instructions provided on each section and cell carefully,\n",
        "5. Complete the coding exercises in sections **Gibbs sampling** and **Mean field**.\n",
        "    \n",
        "**Important**: You are allowed to collaborate with other students in both the math and coding parts of this assignment. However, the answers provided here must reflect your individual work. For that reason, you are not allowed to share this notebook, except for your submission to the TA for grading. **Don't forget to pin and save the version of the notebook you want to be graded on!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02fg3bxiZOMv"
      },
      "source": [
        "## Problem setting\n",
        "\n",
        "Consider the Ising model with binary variables $X_s \\in \\{0,1\\}$ and a factorization of the form:\n",
        "$$\n",
        "p(x; \\eta) = \\frac{1}{Z_p} \\exp \\left( \\sum_{s \\in V} \\eta_s x_s + \\sum_{\\{s,t\\} \\in E} \\eta_{st} x_s x_t \\right).\n",
        "$$\n",
        "We consider the 7 $\\times$7 two-dimensional grid as shown below. Note that we used toroidal (donut-like) boundary conditions to make the problem symmetric. We will consider approximate inference methods to approximate the node marginal moments $\\mu_s := p(X_s = 1)$ in this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LmEnRW4Py-"
      },
      "source": [
        "<center><img src=\"https://imgur.com/Q1u6m5U.png\" alt=\"drawing\" width=\"350\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00t_9oRp8C4q"
      },
      "source": [
        "## Gibbs sampling\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "Using the update equations you found in the theoretical part of the assignment, implement the Gibbs sampling algorithm (with cyclic sequential traversal of the nodes) for $\\eta_{st} = 0.5$ for all edges, and $\\eta_s = (-1)^s$ for all $s \\in \\{1, \\ldots, 49\\}$ (using the node ordering in the figure)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jDGqi5cjNAD"
      },
      "source": [
        "def gibbs_sampling(X0, burn_in, num_epochs):\n",
        "\n",
        "    \"\"\"\n",
        "    Performs Gibbs sampling on the UGM\n",
        "        \n",
        "        Inputs:\n",
        "            X0: [7 x 7] matrix representing the initial state of the grid shown \n",
        "               in the picture. X[0, 0] is the variable numbered 1.\n",
        "            burn_in: [int] burn-in period\n",
        "            num_epochs: [int] number of epochs (one epoch amounts to updating \n",
        "               *each* node once)\n",
        "            \n",
        "        Returns:\n",
        "            samples: [num_epochs x 7 x 7] tensor of generated samples for each \n",
        "                of the epochs after the burn-in period. Here this corresponds to\n",
        "                num-epoch matrices of size 7 x 7.\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Don't forget to include these. It's okay to hardcode values here. No \n",
        "    # need to pass them as function parameters.\n",
        "    eta_s = None\n",
        "    eta_st = None\n",
        "\n",
        "    # TODO\n",
        "    samples = np.random.rand(num_epochs, 7, 7)\n",
        "    \n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybLFB3RJ862v"
      },
      "source": [
        "**Execution**\n",
        "\n",
        "Run a *burn-in period* of 1000 epochs (where **one epoch amounts to updating each node once**). For each of the 5000 *subsequent epochs*, collect a sample vector. Use the 5000 samples to form Monte Carlo estimates $\\hat{\\mu}_s$ of the moments $\\mathbb{E}[X_s]$ at each node. Create a $7 \\times 7$ matrix of the estimated moments.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Note:** We mentioned in class that every update of a node yields a *different* sample in theory, and that one should normally use *all* the available samples (after sufficient mixing) for a Monte Carlo estimate. In this case, that would be $49$ (one for each individual node update) times $5000$ epochs. But note that using all these samples would give almost the exact same estimates, only differing from the boundary conditions during the first and last epoch. Do NOT use the 49x5000 different samples!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7rT9Rfi88sy"
      },
      "source": [
        "# Keep this initialization scheme\n",
        "X0 = 1.0 * (np.random.rand(7, 7) > 0.5)\n",
        "\n",
        "# We run your Gibbs sampling function\n",
        "samples = gibbs_sampling(X0, burn_in=1000, num_epochs=5000)\n",
        "\n",
        "# TODO: Use the samples to perform the Monte Carlo estimates for \\hat{mu}_s\n",
        "mu_hat = np.random.rand(7, 7) # this is dummy code!\n",
        "\n",
        "# Do NOT change these last two lines\n",
        "print(mu_hat)\n",
        "show_matrix(mu_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daXJOP0d89VG"
      },
      "source": [
        "Repeat the experiment $10$ times and output a $7 \\times 7$ matrix of the empirical standard deviation of your estimate at each node (this gives an idea of the variability of your estimates)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjTN-35I_LOv"
      },
      "source": [
        "for trial in range(10):\n",
        "    pass #TODO: do stuff here\n",
        "\n",
        "# TODO: use the result of your 10 trials to compute the empirical standard deviation\n",
        "# of your estimate at each node\n",
        "mu_hat_std = np.random.rand(7, 7)\n",
        "\n",
        "# Do NOT change this last line\n",
        "print(mu_hat_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riXUmhPj9f2C"
      },
      "source": [
        "## Mean field\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "Implement the mean field updates you derived in the theoretical part for the same mode. Use the same $\\eta_s$ and $\\eta_{st}$ as above. Recall we use the notation $q(X_s =1) = \\tau_s$. More specifically, do cyclic coordinate descent on $KL(q || p)$, sequentially updating the parameter $\\tau_s \\in [0,1]$ for $s \\in \\{1, \\ldots, 49\\}$. \n",
        "\n",
        "Let $d(\\tau, \\tau'):= \\frac{1}{49} \\sum_{s=1}^{49} |\\tau_s - \\tau'_s|$ be the average $\\ell_1$ distance between two parameters. Use $d(\\tau^{(k-1)}, \\tau^{k}) <  0.001$ as a stopping criterion for convergence, where $k$ counts the number of **epochs**.  Compute $KL(q || p)-\\log(Z_p)$ as a function of the number of epochs both for debugging purpose and monitor progress. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO0HUJtYA8q3"
      },
      "source": [
        "def distance(tau, tau_prime):\n",
        "    \"\"\"\n",
        "    Computes the average l_1 distance between two parameter configurations\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return 0\n",
        "    \n",
        "def kl_minus_log(tau):\n",
        "    \"\"\"\n",
        "    Computes KL(q(tau) || p) - log(Z_p)\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return 0\n",
        "    \n",
        "def mean_field(tau0, dist_tol):\n",
        "\n",
        "    \"\"\"\n",
        "    Mean field approximation for the UGM\n",
        "        \n",
        "        Inputs:\n",
        "            tau0: [7 x 7] matrix representing the initial value of the parameters\n",
        "               tau_s for each state of the grid shown in the picture. tau[0, 0] \n",
        "               is the parameter associated to the variable numbered 1.\n",
        "            dist_tol: [float] tolerance between epochs. If change in parameter \n",
        "               between two subsequent *epochs* is less than dist_tol, stop.\n",
        "            \n",
        "        Returns:\n",
        "            tau: [7 x 7] matrix of parameters at convergence.\n",
        "            d_hist: [list] list of parameter distance between subsequent epochs.\n",
        "                d_hist[0] = d(tau_0, tau_1) and so on.\n",
        "            kl_hist: [list] list containing KL(q||p) - log(Z_p).\n",
        "                kl_m_log[0] = KL(q(tau_0)||p) - log(Z_p) and so on.\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Don't forget to include these. It's okay to hardcode values here. No \n",
        "    # need to pass them as function parameters.\n",
        "    eta_s = None\n",
        "    eta_st = None\n",
        "\n",
        "    # TODO\n",
        "    tau = tau0\n",
        "    d_hist = []\n",
        "    kl_hist = [kl_minus_log(tau)]\n",
        "    \n",
        "    return tau, d_hist, kl_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J6_RT61HHBi"
      },
      "source": [
        "**Execution**\n",
        "\n",
        "Run your algorithm and plot the mean field estimated moments. Plot the behavior of $d(\\tau^{(k-1)}, \\tau^{k})$ and $KL(q(\\tau^k) || p) -\\log(Z_p)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNRi2byGJlAt"
      },
      "source": [
        "# Keep this initialization for reproducibility\n",
        "tau0 = 0.5 * np.ones((7, 7))\n",
        "\n",
        "# We run your mean field function\n",
        "tau, d_hist, kl_hist = mean_field(tau0, dist_tol=0.001)\n",
        "\n",
        "# Do NOT change these last lines\n",
        "print(tau)\n",
        "show_matrix(tau)\n",
        "plot_list(d_hist, 'Distance')\n",
        "plot_list(kl_hist, 'KL(q||p) - log(Z_p)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhbQ04-pCR9j"
      },
      "source": [
        "Try $5$ different initializations for $\\tau$ and compute $d(\\hat{\\tau}_s, \\hat{\\mu}_s)$ between the mean field estimated moments $\\hat{\\tau}_s$ and the Gibbs estimates $\\hat{\\mu}_s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mapgZ3kCSW_"
      },
      "source": [
        "for trial in range(5):\n",
        "    # 1. Random tau init\n",
        "    # 2. Run mean_field with said initialization\n",
        "    # 3. Print distance between final (mean field) tau and (Gibbs' estimate) \\hat{mu}\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rUWESO1LGmA"
      },
      "source": [
        "**Question:** Is the mean field a good approximation here? Does it get stuck in different local minima?\n",
        "\n",
        "**Answer:** "
      ]
    }
  ]
}